{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49800694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97faf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6abc0a",
   "metadata": {},
   "source": [
    "# I. Import Library and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aab9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20802013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_landmarks(landmarks, dimenson):\n",
    "    landmarks_2d = []\n",
    "    landmarks_3d = []\n",
    "    if dimenson == '2d':\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark.x*1280), int(landmark.y*720)\n",
    "            landmarks_2d.append([x, y])\n",
    "        return landmarks_2d\n",
    "    if dimenson == 'both':\n",
    "        for landmark in landmarks:\n",
    "            x, y = int(landmark.x*1280), int(landmark.y*720)\n",
    "            landmarks_2d.append([x, y])\n",
    "            landmarks_3d.append([x, y, landmark.z])\n",
    "        return landmarks_2d, landmarks_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb607b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    lmks = results.pose_landmarks.landmark\n",
    "    pose_landmarks = [lmks[0], lmks[11], lmks[12], lmks[13], lmks[14], lmks[15], lmks[16], lmks[23], lmks[24], lmks[19], lmks[20]] \n",
    "    pose_landmarks = get_scaled_landmarks(pose_landmarks, '2d')\n",
    "    \n",
    "    cv2.line(image, tuple(pose_landmarks[1]), tuple(pose_landmarks[2]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[1]), tuple(pose_landmarks[3]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[3]), tuple(pose_landmarks[5]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[2]), tuple(pose_landmarks[4]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[4]), tuple(pose_landmarks[6]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[1]), tuple(pose_landmarks[7]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[2]), tuple(pose_landmarks[8]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[7]), tuple(pose_landmarks[8]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[5]), tuple(pose_landmarks[9]), (255, 255, 255), 2)\n",
    "    cv2.line(image, tuple(pose_landmarks[6]), tuple(pose_landmarks[10]), (255, 255, 255), 2)\n",
    "    for lm in pose_landmarks:\n",
    "        cv2.circle(image, (int(lm[0]), int(lm[1])), 4, (0, 0, 255), -1)\n",
    "#     mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS) # Draw pose connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fps(image, prev_frame_time):\n",
    "    new_frame_time = time.time()\n",
    "    fps = int(1/(new_frame_time-prev_frame_time))\n",
    "    cv2.putText(image, f\"fps: {fps}\", (1000, 700), cv2.FONT_HERSHEY_SIMPLEX, 2, (100, 255, 0), 2, cv2.LINE_AA)\n",
    "    return new_frame_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa95206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint_angle(a, b, c):\n",
    "    angle = np.abs(np.arctan2(c.y-b.y, c.x-b.x) - np.arctan2(a.y-b.y, a.x-b.x))\n",
    "    if angle > np.pi:\n",
    "        angle = 2*np.pi-angle\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bd021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_angles(landmarks):\n",
    "    nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "    right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "    right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "    right_ear = landmarks[mp_pose.PoseLandmark.RIGHT_EAR.value]\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "    left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "    left_ear = landmarks[mp_pose.PoseLandmark.LEFT_EAR.value]\n",
    "    right_elbow_angle = get_joint_angle(right_shoulder, right_elbow, right_wrist)\n",
    "    righ_shoulders_angle = get_joint_angle(right_elbow, right_shoulder, left_shoulder)\n",
    "    left_elbow_angle = get_joint_angle(left_shoulder, left_elbow, left_wrist)\n",
    "    left_shoulders_angle = get_joint_angle(left_elbow, left_shoulder, right_shoulder)\n",
    "    nose_angle = get_joint_angle(left_shoulder, nose, right_shoulder)\n",
    "    left_ear_angle = get_joint_angle(left_shoulder, left_ear, right_shoulder)\n",
    "    right_ear_angle = get_joint_angle(left_shoulder, right_ear, right_shoulder)\n",
    "    angles = [right_elbow_angle, righ_shoulders_angle, left_elbow_angle, left_shoulders_angle, nose_angle, left_ear_angle, right_ear_angle]\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee7d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_landmarks(results):\n",
    "    size_landmarks = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark[:23]]).flatten() if results.pose_landmarks else np.zeros(4*23)\n",
    "    world_landmarks =  np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_world_landmarks.landmark[:23]]).flatten() if results.pose_world_landmarks else np.zeros(4*23)\n",
    "    angles = np.array(get_all_angles(results.pose_landmarks.landmark)) if results.pose_landmarks else np.zeros(4)\n",
    "    landmarks = np.concatenate([size_landmarks, world_landmarks, angles])\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b271bb9",
   "metadata": {},
   "source": [
    "# II. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c21cd",
   "metadata": {},
   "source": [
    "#### Create folders to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113737f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('..\\Data\\Landmark Data')\n",
    "VIDEO_PATH = os.path.join('..\\Data\\Video Data')\n",
    "actions = np.array(['non_cheating', 'cheating'])\n",
    "\n",
    "# for action in actions:\n",
    "#     for sequence in range(1, 721):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a4377",
   "metadata": {},
   "source": [
    "#### Extracts landmarks to created folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff243ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_frame_time = 0\n",
    "\n",
    "for action in actions:\n",
    "    action_path = os.path.join(VIDEO_PATH, action)\n",
    "    for id_sequence, video_name in enumerate(os.listdir(action_path)):\n",
    "        video_path = os.path.join(action_path, video_name)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_num = 0\n",
    "        with mp_pose.Pose() as pose:\n",
    "            while cap.isOpened():\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Video end\")\n",
    "                    break\n",
    "\n",
    "                frame_num += 1\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, pose)\n",
    "\n",
    "                # Draw landmarks\n",
    "                if results.pose_landmarks:\n",
    "                    draw_landmarks(image, results) \n",
    "\n",
    "                frame_landmarks = get_frame_landmarks(results)\n",
    "                print(frame_landmarks)\n",
    "                frame_landmarks_path = os.path.join(DATA_PATH, action, str(id_sequence+1), str(frame_num))\n",
    "                np.save(frame_landmarks_path, frame_landmarks)\n",
    "\n",
    "                #Show fps\n",
    "                prev_frame_time = show_fps(image, prev_frame_time)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f29df9",
   "metadata": {},
   "source": [
    "#### Extract landmarks data from folders to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e2e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "sequences = []\n",
    "labels = []\n",
    "for action in actions:\n",
    "    for sequence in range(720):\n",
    "        window = []\n",
    "        for frame_num in range(30):\n",
    "            frame_landmarks_path = os.path.join(DATA_PATH, action, str(sequence+1), str(frame_num+1))\n",
    "            frame_landmarks = np.load(f\"{frame_landmarks_path}.npy\")\n",
    "            window.append(frame_landmarks)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# np.save('final_sequence', sequences)\n",
    "# np.save('final_labels', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd77f4",
   "metadata": {},
   "source": [
    "#### Split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('final_sequence.npy')\n",
    "y = np.load('final_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faeed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X. shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23547bff",
   "metadata": {},
   "source": [
    "# III. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e897a7",
   "metadata": {},
   "source": [
    "#### Model networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 191)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc82926",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320b235",
   "metadata": {},
   "source": [
    "#### Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af17577",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d1c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "ax = plt.figure().gca()\n",
    "\n",
    "plt.plot(np.arange(1,51), history.history['categorical_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('train_accuracy')\n",
    "plt.xlabel('epoch')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30096a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('train_loss')\n",
    "plt.xlabel('epoch')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('saved_pose.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7bdaf",
   "metadata": {},
   "source": [
    "#### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71585ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26992f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('720-data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c36b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c70146",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.argmax(y_res, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(ytrue, yhat)).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796681f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee0dae",
   "metadata": {},
   "source": [
    "# IV. Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_frame_time = 0\n",
    "input_sequence = []\n",
    "predictions = []\n",
    "cap = cv2.VideoCapture(\"cheat_2022-04-11 113657.mp4\")\n",
    "# cap = cv2.VideoCapture(1)\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "cap.set(3, 1280)\n",
    "cap.set(4, 720)\n",
    "count = 0\n",
    "frame_num = 0\n",
    "# result = cv2.VideoWriter('side4.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 15, (1280, 720))\n",
    "\n",
    "with mp_pose.Pose() as pose:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Can't get frame!\")\n",
    "            break\n",
    "\n",
    "        frame_num += 1\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "\n",
    "        #Draw landmarks\n",
    "        if results.pose_landmarks:\n",
    "            draw_landmarks(image, results)\n",
    "\n",
    "        frame_landmarks = get_frame_landmarks(results)\n",
    "        input_sequence.append(frame_landmarks)\n",
    "        input_sequence = input_sequence[-30:]\n",
    "        if len(input_sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(input_sequence, axis=0))[0]\n",
    "            cheating_prob = round(res[1], 2)\n",
    "            cv2.putText(image, \"Cheating probs: \"+str(cheating_prob), (0, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.5, ((255, 0, 0)), 2, cv2.LINE_AA)\n",
    "            if cheating_prob > 0.8:\n",
    "                cv2.putText(image, \"Warning: suspicous behavior\", (7, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (45, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        #Show fps\n",
    "        prev_frame_time = show_fps(image, prev_frame_time)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54710d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:yolov4-gpu]",
   "language": "python",
   "name": "conda-env-yolov4-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
